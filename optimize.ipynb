{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,TimestampType,StringType,LongType\n",
    "\n",
    "# Define the common schema\n",
    "answer_schema = StructType([\\\n",
    "                    StructField(\"question_id\", LongType(),True),\\\n",
    "                    StructField(\"element\", StringType(),True),\\\n",
    "                    StructField(\"creation_date\", TimestampType(),True),\\\n",
    "                    StructField(\"title\", StringType(),True),\\\n",
    "                    StructField(\"accepted_answer_id\", LongType(),True),\\\n",
    "                    StructField(\"comments\", StringType(),True),\\\n",
    "                    StructField(\"user_id\", LongType(),True),\\\n",
    "                    StructField(\"views\", LongType(),True)\\\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimize the query plan\n",
    "\n",
    "Suppose we want to compose query in which we get for each question also the number of answers to this question for each month. See the query below which does that in a suboptimal way and try to rewrite it to achieve a more optimal plan.\n",
    "'''\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, month\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('Optimize I').getOrCreate()\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "answers_input_path = os.path.join(project_path, 'data/answers')\n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'data/questions')\n",
    "\n",
    "answersDF = spark.read.option(\"header\", \"true\").option('path', answers_input_path).load()\n",
    "\n",
    "questionsDF = spark.read.option('path', questions_input_path).schema(answer_schema).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|question_id|       creation_date|               title|month|cnt|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|     155989|2014-12-31 17:59:...|Frost bubble form...|    2|  1|\n",
      "|     155989|2014-12-31 17:59:...|Frost bubble form...|   12|  1|\n",
      "|     155990|2014-12-31 18:51:...|The abstract spac...|    1|  1|\n",
      "|     155990|2014-12-31 18:51:...|The abstract spac...|   12|  1|\n",
      "|     155992|2014-12-31 19:44:...|centrifugal force...|   12|  1|\n",
      "|     155993|2014-12-31 19:56:...|How can I estimat...|    1|  1|\n",
      "|     155995|2014-12-31 21:16:...|Why should a solu...|    1|  3|\n",
      "|     155996|2014-12-31 22:06:...|Why do we assume ...|    1|  2|\n",
      "|     155996|2014-12-31 22:06:...|Why do we assume ...|    2|  1|\n",
      "|     155996|2014-12-31 22:06:...|Why do we assume ...|   11|  1|\n",
      "|     155997|2014-12-31 22:26:...|Why do square sha...|    1|  3|\n",
      "|     155999|2014-12-31 23:01:...|Diagonalizability...|    1|  1|\n",
      "|     156008|2015-01-01 00:48:...|Capturing a light...|    1|  2|\n",
      "|     156008|2015-01-01 00:48:...|Capturing a light...|   11|  1|\n",
      "|     156016|2015-01-01 02:31:...|The interference ...|    1|  1|\n",
      "|     156020|2015-01-01 03:19:...|What is going on ...|    1|  1|\n",
      "|     156021|2015-01-01 03:21:...|How to calculate ...|    2|  1|\n",
      "|     156022|2015-01-01 03:55:...|Advice on Major S...|    1|  1|\n",
      "|     156025|2015-01-01 04:32:...|Deriving the Cano...|    1|  1|\n",
      "|     156026|2015-01-01 04:49:...|Does Bell's inequ...|    1|  3|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 8.934884309768677 seconds ---\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Answers aggregation\n",
    "\n",
    "Here we : get number of answers per question per month\n",
    "'''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "answers_month = answersDF.withColumn('month', month('creation_date'))\n",
    "\n",
    "answers_month = answers_month.groupBy('question_id', 'month').agg(count('*').alias('cnt'))\n",
    "\n",
    "resultDF = questionsDF.join(answers_month, 'question_id').select('question_id', 'creation_date', 'title', 'month', 'cnt')\n",
    "\n",
    "resultDF.orderBy('question_id', 'month').show()\n",
    "\n",
    "'''\n",
    "Task:\n",
    "\n",
    "see the query plan of the previous result and rewrite the query to optimize it\n",
    "'''\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [question_id#12L, creation_date#14, title#15, month#28, cnt#44L]\n",
      "+- *(3) BroadcastHashJoin [question_id#12L], [question_id#0L], Inner, BuildRight\n",
      "   :- *(3) Project [question_id#12L, creation_date#14, title#15]\n",
      "   :  +- *(3) Filter isnotnull(question_id#12L)\n",
      "   :     +- *(3) ColumnarToRow\n",
      "   :        +- FileScan parquet [question_id#12L,creation_date#14,title#15] Batched: true, DataFilters: [isnotnull(question_id#12L)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/SpringBoard-DataEngineer/Spark-OptimizationProject/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true])), [id=#135]\n",
      "      +- *(2) HashAggregate(keys=[question_id#0L, month#28], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(question_id#0L, month#28, 200), true, [id=#131]\n",
      "            +- *(1) HashAggregate(keys=[question_id#0L, month#28], functions=[partial_count(1)])\n",
      "               +- *(1) Project [question_id#0L, month(cast(creation_date#2 as date)) AS month#28]\n",
      "                  +- *(1) Filter isnotnull(question_id#0L)\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet [question_id#0L,creation_date#2] Batched: true, DataFilters: [isnotnull(question_id#0L)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/SpringBoard-DataEngineer/Spark-OptimizationProject/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|question_id|       creation_date|               title|month|cnt|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|     155989|2014-12-31 17:59:...|Frost bubble form...|    2|  1|\n",
      "|     155989|2014-12-31 17:59:...|Frost bubble form...|   12|  1|\n",
      "|     155990|2014-12-31 18:51:...|The abstract spac...|    1|  1|\n",
      "|     155990|2014-12-31 18:51:...|The abstract spac...|   12|  1|\n",
      "|     155992|2014-12-31 19:44:...|centrifugal force...|   12|  1|\n",
      "|     155993|2014-12-31 19:56:...|How can I estimat...|    1|  1|\n",
      "|     155995|2014-12-31 21:16:...|Why should a solu...|    1|  3|\n",
      "|     155996|2014-12-31 22:06:...|Why do we assume ...|    1|  2|\n",
      "|     155996|2014-12-31 22:06:...|Why do we assume ...|    2|  1|\n",
      "|     155996|2014-12-31 22:06:...|Why do we assume ...|   11|  1|\n",
      "|     155997|2014-12-31 22:26:...|Why do square sha...|    1|  3|\n",
      "|     155999|2014-12-31 23:01:...|Diagonalizability...|    1|  1|\n",
      "|     156008|2015-01-01 00:48:...|Capturing a light...|    1|  2|\n",
      "|     156008|2015-01-01 00:48:...|Capturing a light...|   11|  1|\n",
      "|     156016|2015-01-01 02:31:...|The interference ...|    1|  1|\n",
      "|     156020|2015-01-01 03:19:...|What is going on ...|    1|  1|\n",
      "|     156021|2015-01-01 03:21:...|How to calculate ...|    2|  1|\n",
      "|     156022|2015-01-01 03:55:...|Advice on Major S...|    1|  1|\n",
      "|     156025|2015-01-01 04:32:...|Deriving the Cano...|    1|  1|\n",
      "|     156026|2015-01-01 04:49:...|Does Bell's inequ...|    1|  3|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 4.185990810394287 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "\n",
    "answers_month = answersDF.withColumn('month', month('creation_date'))\n",
    "\n",
    "# Repartition by Month to avoid shuffling\n",
    "\n",
    "answers_month = answers_month.repartition(col(\"month\"))\n",
    "\n",
    "answers_month = answers_month.groupBy('question_id', 'month').agg(count('*').alias('cnt'))\n",
    "\n",
    "resultDF = questionsDF.join(answers_month, 'question_id').select('question_id', 'creation_date', 'title', 'month', 'cnt')\n",
    "\n",
    "resultDF.orderBy('question_id', 'month').show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [question_id#12L, creation_date#14, title#15, month#86, cnt#102L]\n",
      "   +- BroadcastHashJoin [question_id#12L], [question_id#0L], Inner, BuildRight\n",
      "      :- Project [question_id#12L, creation_date#14, title#15]\n",
      "      :  +- Filter isnotnull(question_id#12L)\n",
      "      :     +- FileScan parquet [question_id#12L,creation_date#14,title#15] Batched: true, DataFilters: [isnotnull(question_id#12L)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/SpringBoard-DataEngineer/Spark-OptimizationProject/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true])), [id=#313]\n",
      "         +- HashAggregate(keys=[question_id#0L, month#86], functions=[count(1)])\n",
      "            +- HashAggregate(keys=[question_id#0L, month#86], functions=[partial_count(1)])\n",
      "               +- Exchange hashpartitioning(month#86, 200), false, [id=#307]\n",
      "                  +- Project [question_id#0L, month(cast(creation_date#2 as date)) AS month#86]\n",
      "                     +- Filter isnotnull(question_id#0L)\n",
      "                        +- FileScan parquet [question_id#0L,creation_date#2] Batched: true, DataFilters: [isnotnull(question_id#0L)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/SpringBoard-DataEngineer/Spark-OptimizationProject/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
